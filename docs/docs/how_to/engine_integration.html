





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Integration with LLM Engine &mdash; XGrammar 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Portable API" href="portable_api.html" />
    <link rel="prev" title="EBNF-Guided Generation" href="ebnf_guided_generation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://xgrammar.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/xgrammar>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="sidetitle" alt="Documentation Home"> XGrammar
          

          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../start/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../start/quick_start.html">Quick Start</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How To</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="json_generation.html">JSON Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="ebnf_guided_generation.html">EBNF-Guided Generation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Integration with LLM Engine</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install-xgrammar">Install XGrammar</a></li>
<li class="toctree-l2"><a class="reference internal" href="#high-level-flow">High-Level Flow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#xgr-tokenizerinfo">xgr.TokenizerInfo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#xgr-grammarcompiler">xgr.GrammarCompiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="#xgr-compiledgrammar">xgr.CompiledGrammar</a></li>
<li class="toctree-l3"><a class="reference internal" href="#xgr-grammarmatcher">xgr.GrammarMatcher</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bitmasking-logits-in-auto-regressive-generation">Bitmasking Logits in Auto-regressive Generation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#structured-generation-for-batched-inference">Structured Generation for Batched Inference</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="portable_api.html">Portable API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/python/index.html">xgrammar</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- XGrammar -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Integration with LLM Engine</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/xgrammar/edit/main/docs/how_to/engine_integration.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="integration-with-llm-engine">
<span id="how-to-engine-integration"></span><h1>Integration with LLM Engine<a class="headerlink" href="#integration-with-llm-engine" title="Permalink to this heading">¶</a></h1>
<p>XGrammar enables efficient structured generation. In this tutorial, we go over the key components
of XGrammar and how to integrate XGrammar into an LLM engine.</p>
<p>We first lay out the concepts in <a class="reference internal" href="#how-to-engine-integration-flow"><span class="std std-ref">High-Level Flow</span></a>.
We then demonstrate how XGrammar enables
<a class="reference internal" href="#how-to-engine-integration-batched"><span class="std std-ref">Structured Generation for Batched Inference</span></a>.</p>
<p>The code snippets below are actual runnable code as we simulate the LLM generation.</p>
<section id="install-xgrammar">
<h2>Install XGrammar<a class="headerlink" href="#install-xgrammar" title="Permalink to this heading">¶</a></h2>
<p><a class="reference internal" href="../start/install.html#installation-prebuilt-package"><span class="std std-ref">XGrammar</span></a> is available via pip.
It is always recommended to install it in an isolated conda virtual environment.</p>
</section>
<section id="high-level-flow">
<span id="how-to-engine-integration-flow"></span><h2>High-Level Flow<a class="headerlink" href="#high-level-flow" title="Permalink to this heading">¶</a></h2>
<p>In this section, we go over the key components of XGrammar when integrating it into an LLM engine
for structured generation.</p>
<p>First, import necessary libraries for the tutorial.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">xgrammar</span> <span class="k">as</span> <span class="nn">xgr</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span>
</pre></div>
</div>
<section id="xgr-tokenizerinfo">
<h3>xgr.TokenizerInfo<a class="headerlink" href="#xgr-tokenizerinfo" title="Permalink to this heading">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code> is a per-model construct that encapsulates tokenizer information, including
all its vocabulary. There are several ways of instantiating it, and the most convenient way
is using an <code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code>. Note that for some models, <code class="docutils literal notranslate"><span class="pre">AutoConfig.vocab_size</span></code> can be larger
than <code class="docutils literal notranslate"><span class="pre">AutoTokenizer.vocab_size</span></code> due to paddings, with the former being the shape of the model’s
logits. To be safe, always pass in the former when instantiating <code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get tokenizer info</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="c1"># This can be larger than tokenizer.vocab_size due to paddings</span>
<span class="n">full_vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">tokenizer_info</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">full_vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="xgr-grammarcompiler">
<h3>xgr.GrammarCompiler<a class="headerlink" href="#xgr-grammarcompiler" title="Permalink to this heading">¶</a></h3>
<p>With an <code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code>, we can instantiate an <code class="docutils literal notranslate"><span class="pre">xgr.GrammarCompiler</span></code>. This is a construct
that compiles a grammar according to the model’s tokenizer info. Therefore, for each model, you
can use the same <code class="docutils literal notranslate"><span class="pre">xgr.GrammarCompiler</span></code> persistently, as it can compile different grammars for
the same <code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code>. Note that the <code class="docutils literal notranslate"><span class="pre">compiler</span></code> behavior can be configured with
<code class="docutils literal notranslate"><span class="pre">max_threads</span></code> for multithreading, and <code class="docutils literal notranslate"><span class="pre">enable_cache</span></code> (defaults to true) for caching
compiled grammars.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">compiler</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">GrammarCompiler</span><span class="p">(</span><span class="n">tokenizer_info</span><span class="p">,</span> <span class="n">max_threads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="xgr-compiledgrammar">
<h3>xgr.CompiledGrammar<a class="headerlink" href="#xgr-compiledgrammar" title="Permalink to this heading">¶</a></h3>
<p>Then, using the <code class="docutils literal notranslate"><span class="pre">xgr.GrammarCompiler</span></code>, we can compile a grammar, with the result being an
<code class="docutils literal notranslate"><span class="pre">xgr.CompiledGrammar</span></code>. Here we use a built-in JSON grammar. For other grammars, see
<a class="reference internal" href="json_generation.html#how-to-json-generation"><span class="std std-ref">JSON Generation</span></a> and <a class="reference internal" href="ebnf_guided_generation.html#how-to-ebnf-generation"><span class="std std-ref">EBNF-Guided Generation</span></a>.
Every thing we have seen up to now are per-model (rather than per-generation).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">compiled_grammar</span><span class="p">:</span> <span class="n">xgr</span><span class="o">.</span><span class="n">CompiledGrammar</span> <span class="o">=</span> <span class="n">compiler</span><span class="o">.</span><span class="n">compile_builtin_json_grammar</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="xgr-grammarmatcher">
<h3>xgr.GrammarMatcher<a class="headerlink" href="#xgr-grammarmatcher" title="Permalink to this heading">¶</a></h3>
<p>With the compiled grammar, we can instantiate a <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code>. It is the main construct
an LLM engine interacts with that maintains the state of the structured generation. Note that
each request should have its own <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code> since each has a different generation state,
as we will see in <a class="reference internal" href="#how-to-engine-integration-batched"><span class="std std-ref">Structured Generation for Batched Inference</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate grammar matcher with the compiled grammar</span>
<span class="n">matcher</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">GrammarMatcher</span><span class="p">(</span><span class="n">compiled_grammar</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="bitmasking-logits-in-auto-regressive-generation">
<h3>Bitmasking Logits in Auto-regressive Generation<a class="headerlink" href="#bitmasking-logits-in-auto-regressive-generation" title="Permalink to this heading">¶</a></h3>
<p>Now we simulate a single-request auto-regressive generation. See later section for
<a class="reference internal" href="#how-to-engine-integration-batched"><span class="std std-ref">Structured Generation for Batched Inference</span></a>.</p>
<p>First, we pre-allocate a token bitmask with <code class="docutils literal notranslate"><span class="pre">xgr.allocate_token_bitmask()</span></code>,
which is essentially a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">vocab_size)</span></code>. You can also
use your own implementation for allocating a bitmask.</p>
<p>In each auto-regressive step, we fill the token bitmask according to the current state
of the matcher with <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher.fill_next_token_bitmask()</span></code>. Then, we apply the bitmask
into the model’s logits with <code class="docutils literal notranslate"><span class="pre">xgr.apply_token_bitmask_inplace()</span></code>, which calls a CUDA kernel
if <code class="docutils literal notranslate"><span class="pre">logits</span></code> is on CUDA (recommended), otherwise a CPU implementation.</p>
<p>After masking, the logits for illegal tokens are set to negative infinity, so that
we will never sample them. After sampling the token, update the <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code>’s state with
<code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher.accept_token()</span></code>. Finally, use  <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher.reset()</span></code> to prepare
for the next generation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Here we simulate a valid sampled response</span>
<span class="n">sim_sampled_response</span> <span class="o">=</span> <span class="s1">&#39;{ &quot;library&quot;: &quot;xgrammar&quot; }&lt;|endoftext|&gt;&#39;</span>
<span class="n">sim_sampled_token_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sim_sampled_response</span><span class="p">)</span>

<span class="c1"># Allocate a token bitmask</span>
<span class="n">token_bitmask</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">allocate_token_bitmask</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">tokenizer_info</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>

<span class="c1"># Each loop iteration is a simulated auto-regressive step</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sim_token_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sim_sampled_token_ids</span><span class="p">):</span>
    <span class="c1"># LLM inference to get logits, here we use randn to simulate.</span>
    <span class="c1"># logits is a tensor of shape (full_vocab_size,) on GPU</span>
    <span class="c1"># logits = LLM.inference()</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">full_vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># Apply bitmask to logits to mask invalid tokens</span>
    <span class="n">matcher</span><span class="o">.</span><span class="n">fill_next_token_bitmask</span><span class="p">(</span><span class="n">token_bitmask</span><span class="p">)</span>
    <span class="n">xgr</span><span class="o">.</span><span class="n">apply_token_bitmask_inplace</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">token_bitmask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

    <span class="c1"># Sample next token</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">full_vocab_size</span><span class="p">)),</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">)</span>

    <span class="c1"># Accept token from matcher to update its state, so that the next bitmask</span>
    <span class="c1"># generated will enforce the next token to be generated. Assert to make</span>
    <span class="c1"># sure the token is indeed valid. Here we accept the simulated response</span>
    <span class="c1"># assert matcher.accept_token(next_token_id)</span>
    <span class="k">assert</span> <span class="n">matcher</span><span class="o">.</span><span class="n">accept_token</span><span class="p">(</span><span class="n">sim_token_id</span><span class="p">)</span>

<span class="c1"># Since we accepted a stop token `&lt;|endoftext|&gt;`, we have terminated</span>
<span class="k">assert</span> <span class="n">matcher</span><span class="o">.</span><span class="n">is_terminated</span><span class="p">()</span>

<span class="c1"># Reset to be ready for the next auto-regressive generation</span>
<span class="n">matcher</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="structured-generation-for-batched-inference">
<span id="how-to-engine-integration-batched"></span><h2>Structured Generation for Batched Inference<a class="headerlink" href="#structured-generation-for-batched-inference" title="Permalink to this heading">¶</a></h2>
<p>The code snippets above assume a single request generation.
This section demonstrates how the same concept works with batched generation.</p>
<p>First, follow the exact same steps above for the per-model constructs
<code class="docutils literal notranslate"><span class="pre">xgr.TokenizerInfo</span></code> and <code class="docutils literal notranslate"><span class="pre">xgr.GrammarCompiler</span></code>. Say each request needs
to generate a valid JSON.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">xgrammar</span> <span class="k">as</span> <span class="nn">xgr</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoConfig</span>

<span class="c1"># Get tokenizer info</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;Qwen/Qwen2.5-0.5B-Instruct&quot;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">AutoConfig</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="c1"># This can be larger than tokenizer.vocab_size due to paddings</span>
<span class="n">full_vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">tokenizer_info</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">TokenizerInfo</span><span class="o">.</span><span class="n">from_huggingface</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="n">full_vocab_size</span><span class="p">)</span>

<span class="c1"># Compile a JSON grammar</span>
<span class="n">compiler</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">GrammarCompiler</span><span class="p">(</span><span class="n">tokenizer_info</span><span class="p">,</span> <span class="n">max_threads</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">compiled_grammar</span><span class="p">:</span> <span class="n">xgr</span><span class="o">.</span><span class="n">CompiledGrammar</span> <span class="o">=</span> <span class="n">compiler</span><span class="o">.</span><span class="n">compile_builtin_json_grammar</span><span class="p">()</span>
</pre></div>
</div>
<p>Now, we need to maintain an <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code> for each request in the batch, since
each has a different generation state. Note that each request in the batch can follow a different
<code class="docutils literal notranslate"><span class="pre">xgr.CompiledGrammar</span></code>, but here for simplicity, they are all just following the general
JSON grammar.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">matchers</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">xgr</span><span class="o">.</span><span class="n">GrammarMatcher</span><span class="p">(</span><span class="n">compiled_grammar</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
<span class="p">]</span>
<span class="n">token_bitmask</span> <span class="o">=</span> <span class="n">xgr</span><span class="o">.</span><span class="n">allocate_token_bitmask</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tokenizer_info</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
<p>We simulate an auto-regressive generation of batched inference. Note that here we
assume the generation lengths of the two requests are the same for simplicity. But
it should be easy to generalize based on how your engine supports batched inference.
The key difference from single-request generation is that, in batched-request generation,
each request has its own <code class="docutils literal notranslate"><span class="pre">xgr.GrammarMatcher</span></code> to maintain.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sim_sampled_responses</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;{&quot;name&quot;: &quot;a&quot;}&lt;|endoftext|&gt;&#39;</span><span class="p">,</span> <span class="s1">&#39;{&quot;name&quot;: &quot;b&quot;}&lt;|endoftext|&gt;&#39;</span><span class="p">]</span>
<span class="n">sim_sampled_token_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">response</span><span class="p">)</span> <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">sim_sampled_responses</span><span class="p">]</span>

<span class="c1"># Each loop iteration is a simulated auto-regressive step</span>
<span class="k">for</span> <span class="n">loop_iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sim_sampled_token_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">])):</span>
    <span class="c1"># LLM batched inference to get logits, here we use randn to simulate</span>
    <span class="c1"># Now, logits is a tensor of shape (batch_size, full_vocab_size) on GPU</span>
    <span class="c1"># logits = LLM.inference()</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">full_vocab_size</span><span class="p">)</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="c1"># This for loop is parallelizable using threading.Thread. But estimate</span>
    <span class="c1"># the overhead in your engine.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">matchers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_next_token_bitmask</span><span class="p">(</span><span class="n">token_bitmask</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="n">xgr</span><span class="o">.</span><span class="n">apply_token_bitmask_inplace</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">token_bitmask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

    <span class="c1"># Sample next token</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="n">next_token_ids</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">full_vocab_size</span><span class="p">)),</span> <span class="n">p</span><span class="o">=</span><span class="n">probs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Update the matcher for each request</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="c1"># Here we accept the simulated response</span>
        <span class="c1"># assert matchers[i].accept_token(next_token_ids[i])</span>
        <span class="n">matchers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">accept_token</span><span class="p">(</span><span class="n">sim_sampled_token_ids</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">loop_iter</span><span class="p">])</span>

<span class="c1"># In our simulated case, all requests should have terminated since we accepted</span>
<span class="c1"># a stop token `&lt;|endoftext|&gt;`</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">matchers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">is_terminated</span><span class="p">()</span>
    <span class="c1"># Reset to be ready for the next generation</span>
    <span class="n">matchers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="portable_api.html" class="btn btn-neutral float-right" title="Portable API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ebnf_guided_generation.html" class="btn btn-neutral float-left" title="EBNF-Guided Generation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2024 XGrammar</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>